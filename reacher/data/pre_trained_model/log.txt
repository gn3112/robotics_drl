Loss training	Loss validation
2.3133745193481445	2.5984129905700684
2.604637622833252	2.5909225940704346
2.6726808547973633	2.5806281566619873
2.7743914127349854	2.588874101638794
2.59580135345459	2.601936101913452
2.5870471000671387	2.6107749938964844
2.54226016998291	2.621926784515381
2.428546905517578	2.6367416381835938
2.5698845386505127	2.654245615005493
2.7713441848754883	2.7068960666656494
2.232977867126465	2.686574935913086
2.2876029014587402	2.7035422325134277
2.561262845993042	2.76491117477417
2.2610630989074707	2.770063877105713
2.1935853958129883	2.794828176498413
2.120192527770996	2.8381094932556152
2.219099998474121	2.865966558456421
2.0473015308380127	2.9090495109558105
2.1036407947540283	2.9036123752593994
2.285660743713379	2.9155001640319824
2.2466752529144287	2.9321906566619873
2.363770008087158	2.997400999069214
2.5120902061462402	2.9689762592315674
2.5400657653808594	3.015558958053589
2.031301259994507	3.043574810028076
2.0740432739257812	3.0694997310638428
2.154510021209717	3.1041259765625
2.136098861694336	3.104508638381958
2.57179594039917	3.1456916332244873
1.9755090475082397	3.145326614379883
1.9762616157531738	3.1548399925231934
1.9520119428634644	3.238083600997925
2.0505542755126953	3.2169485092163086
2.0086636543273926	3.188764810562134
2.031863212585449	3.2891812324523926
2.266089916229248	3.2824127674102783
1.753052830696106	3.293792247772217
2.040253162384033	3.3112120628356934
1.820420265197754	3.262657403945923
1.7412292957305908	3.385993719100952
1.707934021949768	3.401674747467041
1.8080649375915527	3.4140655994415283
1.7898399829864502	3.4488089084625244
1.8825697898864746	3.4528164863586426
1.8084359169006348	3.4943575859069824
1.4555718898773193	3.4385006427764893
2.036884307861328	3.475466728210449
2.0759689807891846	3.5131163597106934
2.078730821609497	3.5610573291778564
1.7104153633117676	3.5318191051483154
1.8588755130767822	3.578282356262207
1.6352488994598389	3.614837646484375
